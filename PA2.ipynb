{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvROd_yDUuVX",
        "outputId": "58ae7615-39cb-4f1a-f908-2802de481c58"
      },
      "source": [
        "# Ayesha Akhtar\n",
        "# Deep Learning PA 2\n",
        "import tensorflow as tf #tensorflow for cnn\n",
        "%matplotlib inline\n",
        "import os.path #filepath\n",
        "import numpy as np #setting columns\n",
        "import pandas as pd #dataframe\n",
        "from pathlib import Path #path\n",
        "from sklearn.model_selection import train_test_split #creating cnn\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import r2_score #r2 score\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('wiki_labels.csv') #open labels file\n",
        "#del df['ID']\n",
        "del df['dob']#remove irrelevant dob\n",
        "del df['dob_str'] #remove irrelevant dob str\n",
        "del df['photo_taken'] #remove irrelevant date\n",
        "del df['gender'] #remove gender\n",
        "del df['name'] #remove name\n",
        "del df['face_location'] #remove face_location\n",
        "del df['face_score'] #remove face_score\n",
        "del df['second_face_score'] #remove\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('wiki_judge_images.zip', 'r') as zip_ref: #upload judge images as zip\n",
        "    zip_ref.extractall(\"target\") #open zip\n",
        "\n",
        "with zipfile.ZipFile('wiki_labeled.zip', 'r') as zip_ref1: #upload labeled imgs as zip\n",
        "    zip_ref1.extractall(\"ExtractLabeled\") #open zip\n",
        "agearr =[]\n",
        "patharr =[]\n",
        "idarr=[]\n",
        "for x in df['age']:\n",
        "    x = int(x);#turn ages into int\n",
        "    agearr.append(x) #add to list\n",
        "    \n",
        "for x in df['ID']:\n",
        "    idarr.append(x); #add IDs to list\n",
        "    \n",
        "for x in df['full_path']:\n",
        "    x =x[1:-1]#remove unnecessary []\n",
        "    x=x.strip(\"'\") #remove quotes\n",
        "    \n",
        "    y = '/content/ExtractLabeled/wiki_labeled/'; #add this to path (Working on Colab Google)\n",
        "    newstr = y+x; #add to path\n",
        "    patharr.append(newstr) #save new path\n",
        "  # /content/ExtractLabeled/wiki_labeled/00/10049200_1891-09-16_1958.png \n",
        "# for x in patharr:\n",
        "#     print(x)\n",
        "    \n",
        "Paths = pd.Series(patharr, name=\"Paths\").astype(str) #save all paths as string series\n",
        "Ages = pd.Series(agearr, name=\"Ages\").astype(np.int) #save all ages as int series\n",
        "\n",
        "Concat = pd.concat([Paths, Ages], axis=1).sample(frac=1.0,random_state=1).reset_index(drop=True) #concat together\n",
        "#print(Concat)\n",
        "#print(df)\n",
        "#ExtractLabeled\\wiki_labeled\\00\\10049200_1891-0...\n",
        "labeledPath = Path('content/ExtractLabeled/wiki_labeled') #get path for labeled images\n",
        "LabelledPics = pd.Series(list(labeledPath.glob('**/*.png')), name =\"LabelledPic\") #find all pics in labeled images\n",
        "\n",
        "#LabelledPics\n",
        "\n",
        "img_df = Concat.sample(10000, random_state=1).reset_index(drop=True) #concat sample\n",
        "train_df, test_df = train_test_split(img_df, train_size = 0.7,shuffle=True, random_state=1 ) #split train and test set\n",
        "\n",
        "trainer = tf.keras.preprocessing.image.ImageDataGenerator( #train set image \n",
        "    rescale = 1./255, #resize image\n",
        "    validation_split=0.2 \n",
        ")\n",
        "tester = tf.keras.preprocessing.image.ImageDataGenerator( #test set image\n",
        "    rescale = 1./255 #resize\n",
        ")\n",
        "\n",
        "train_img = trainer.flow_from_dataframe( #get train images from df\n",
        "    dataframe = train_df, #train df\n",
        "    x_col = \"Paths\", #use paths as x\n",
        "    y_col = \"Ages\", #use ages as y\n",
        "    target_size = (120,120), #size\n",
        "    color_mode = 'rgb', #color intact\n",
        "    class_mode=\"raw\", #class mode\n",
        "    batch_size = 32, #batch size - tested with various\n",
        "    shuffle=True, #shuffle around/randomize\n",
        "    seed = 42,\n",
        "    subset = \"training\"\n",
        ")\n",
        "validate = trainer.flow_from_dataframe( #validation imgs\n",
        "    dataframe = train_df, #get from train ff\n",
        "    x_col = \"Paths\", #use paths as x\n",
        "    y_col = \"Ages\", #use ages as y\n",
        "    target_size = (120,120), #size\n",
        "    color_mode = 'rgb', #color intact intact\n",
        "    class_mode=\"raw\", #class\n",
        "    batch_size = 32, #batch\n",
        "    shuffle=True, #randomize\n",
        "    seed = 42,\n",
        "    subset = \"validation\"\n",
        ")\n",
        "test_img = tester.flow_from_dataframe( #test images\n",
        "    dataframe = test_df, #use test df\n",
        "    x_col = \"Paths\", #x column\n",
        "    y_col = \"Ages\", #y column\n",
        "    target_size = (120,120), #size\n",
        "    color_mode = 'rgb', #color intact\n",
        "    class_mode=\"raw\", #class\n",
        "    batch_size = 32,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "inputs = tf.keras.Input(shape=(120,120,3)) #inputs shape\n",
        "fLayer = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(inputs) #layer 1 with 16 filters and relu\n",
        "fLayer = tf.keras.layers.AveragePooling2D()(fLayer) #max pool/results\n",
        "\n",
        "fLayer = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(fLayer) #layer 2 with 32 filters and relu\n",
        "fLayer = tf.keras.layers.AveragePooling2D()(fLayer) #max pool/results\n",
        "fLayer = tf.keras.layers.GlobalAveragePooling2D()(fLayer) #get average\n",
        "\n",
        "fLayer = tf.keras.layers.Dense(64, activation='relu')(fLayer) #dense layer 1\n",
        "fLayer = tf.keras.layers.Dense(64, activation='relu')(fLayer) #dense layer 2\n",
        "fLayer = tf.keras.layers.Dense(64, activation='relu')(fLayer) #dense layer 3\n",
        "\n",
        "output = tf.keras.layers.Dense(1, activation='linear')(fLayer) #get output in linear form\n",
        "model = tf.keras.Model(inputs=inputs, outputs=output) #create model with inputs and outputs\n",
        "\n",
        "model.compile( #model compiler\n",
        "    optimizer='adam', #use adam\n",
        "    loss = 'mse' #mse\n",
        ")\n",
        "modelResults = model.fit( #get model results fitted\n",
        "    train_img,\n",
        "    validation_data=validate,\n",
        "    epochs=100, #100 epochs test\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor = 'val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True #get best possible weights\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "predicted_age = np.squeeze(model.predict(test_img)) #test with predicted age on wiki label images\n",
        "true_age = test_img.labels #get actual ages\n",
        "\n",
        "rmse = np.sqrt(model.evaluate(test_img, verbose=0)) #determine rsme\n",
        "print(\"RSME: \", rmse) #around 19\n",
        "r2= r2_score(true_age,predicted_age) #get r2 score\n",
        "print(\"R2: \", r2) #around 0\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 5600 validated image filenames.\n",
            "Found 1400 validated image filenames.\n",
            "Found 3000 validated image filenames.\n",
            "Epoch 1/100\n",
            "175/175 [==============================] - 5s 28ms/step - loss: 1132.1778 - val_loss: 645.9075\n",
            "Epoch 2/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 438.6340 - val_loss: 639.3805\n",
            "Epoch 3/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 481.9100 - val_loss: 640.9985\n",
            "Epoch 4/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 415.2037 - val_loss: 627.7242\n",
            "Epoch 5/100\n",
            "175/175 [==============================] - 4s 25ms/step - loss: 434.3864 - val_loss: 619.3061\n",
            "Epoch 6/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 435.3664 - val_loss: 621.8293\n",
            "Epoch 7/100\n",
            "175/175 [==============================] - 4s 26ms/step - loss: 453.3080 - val_loss: 619.0695\n",
            "Epoch 8/100\n",
            "175/175 [==============================] - 4s 26ms/step - loss: 513.1293 - val_loss: 620.7501\n",
            "Epoch 9/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 478.3062 - val_loss: 626.7267\n",
            "Epoch 10/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 452.0225 - val_loss: 640.8224\n",
            "Epoch 11/100\n",
            "175/175 [==============================] - 4s 26ms/step - loss: 462.1080 - val_loss: 626.3781\n",
            "Epoch 12/100\n",
            "175/175 [==============================] - 4s 26ms/step - loss: 443.5638 - val_loss: 617.7620\n",
            "Epoch 13/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 438.0781 - val_loss: 624.7476\n",
            "Epoch 14/100\n",
            "175/175 [==============================] - 4s 26ms/step - loss: 446.3379 - val_loss: 642.6563\n",
            "Epoch 15/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 397.2175 - val_loss: 622.1829\n",
            "Epoch 16/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 452.0085 - val_loss: 636.2217\n",
            "Epoch 17/100\n",
            "175/175 [==============================] - 5s 26ms/step - loss: 502.4109 - val_loss: 620.6602\n",
            "RSME:  19.581243049043213\n",
            "R2:  -0.0031711909270841776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-eCmmposr80"
      },
      "source": [
        "model.save('./mymodel.h5') #save model for judge data"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDmwwPMPk1nN"
      },
      "source": [
        "model = tf.keras.models.load_model('./mymodel.h5') #load model"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_4Yw95wpO0B",
        "outputId": "928bf34b-5b4f-48b1-d822-14be4ea5e000"
      },
      "source": [
        "mynewset = tf.keras.preprocessing.image.ImageDataGenerator( #test set image \n",
        "    rescale = 1./255, #resize image\n",
        "    validation_split=0.2 \n",
        ")\n",
        "\n",
        "df1 = pd.read_csv('wiki_judge.csv') #read in judge info\n",
        "del df1 ['gender'] #remove irrelevant gender column\n",
        "del df1['face_score'] #remove irrelevant face_score column\n",
        "del df1['second_face_score'] #remove second_face_score\n",
        "\n",
        "#/content/target/wiki_judge_images/1.png\n",
        "myarr=[]\n",
        "for x in df1['full_path']: #for all paths\n",
        "  y = '/content/target/wiki_judge_images/' #add this to path due to working on Colab Google\n",
        "  x =x[1:-1]#remove unnecessary []\n",
        "  x=x.strip(\"'\") #remove quotes\n",
        "  newstr = y+x #combine strings\n",
        "  myarr.append(newstr) #save new path\n",
        "ID =[];\n",
        "for x in df1['ID']: #for all IDs\n",
        "  ID.append(x) #save id\n",
        "e_dataframe = pd.DataFrame( #put IDS and paths into dataframe\n",
        "    {'ID': ID,\n",
        "     'Path': myarr,})\n",
        "\n",
        "\n",
        "train = mynewset.flow_from_dataframe( #get test images from df\n",
        "    dataframe = e_dataframe, #judge imgs df\n",
        "    x_col = \"Path\", #use paths as x\n",
        "    y_col = \"ID\", #use ID as y - this isn't processed so it's fine to have this here\n",
        "    target_size = (120,120), #size\n",
        "    color_mode = 'rgb', #color intact\n",
        "    class_mode=\"raw\", #class since we're dealing with ints\n",
        "    batch_size = 32,\n",
        "    shuffle=False\n",
        ")\n",
        "predicted_age = np.squeeze(model.predict(train)) #get all predicted ages\n",
        "\n",
        "a_zip = zip(train.labels, predicted_age) #zip predicted ages with their appropiate IDs\n",
        "\n",
        "zipped_list = list(a_zip) #turn zipped list into list\n",
        "# for x in zipped_list:\n",
        "#   print(x)\n",
        "fields = ['ID', 'Age']\n",
        "with open('submission.csv', 'w') as f: # open submission.csv and write in it      \n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerow(fields) #headers\n",
        "    write.writerows(zipped_list)# IDS and predicted ages"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1409 validated image filenames.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pft2fTeBVfNg"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVtRqyVVh3A"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeyCEwO2ViVn"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}